{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Final Report for Leeds Road Safety Plan for 2021\n",
    "---\n",
    "Date: 25.02.2021\n",
    "Group 9: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Required Libraries\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                     # used to store the datasets\n",
    "import numpy as np                      # used for numerical calculations on individual column\n",
    "import matplotlib.pyplot as plt         # plotting \n",
    "import seaborn as sns                   # plotting numerical against categorical variables\n",
    "import textwrap                         # prettify axis labelling\n",
    "import json                             # data transfer to json format\n",
    "import os                               # automates saving of export files (figures, numerical summaries, ...)\n",
    "import folium                           # spatial visualisation"
   ]
  },
  {
   "source": [
    "## Constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path lookup dictionary to store the relative paths from the directory containing the jupyter notebooks to important directories in the project\n",
    "PATH = {}\n",
    "PATH[\"data_raw\"] = \"../data/raw/\"\n",
    "PATH[\"data_interim\"] = \"../data/interim/\"\n",
    "PATH[\"data_processed\"] = \"../data/processed/\"\n",
    "PATH[\"data_external\"] = \"../data/external/\"\n",
    "PATH[\"references\"] = \"../data/references/\"\n",
    "PATH['reports'] = {}\n",
    "PATH['reports']['accidents'] = '../reports/accidents/'\n",
    "PATH['reports']['casualties'] = '../reports/casualties/'\n",
    "PATH['reports']['vehicles'] = '../reports/vehicles/'\n",
    "\n",
    "\n",
    "# filename lookup dictionary storing the most relevant filenames\n",
    "FILENAME = {}\n",
    "FILENAME[\"accidents\"] = \"Road Safety Data - Accidents 2019.csv\"\n",
    "FILENAME[\"casualties\"] = \"Road Safety Data - Casualties 2019.csv\"\n",
    "FILENAME[\"vehicles\"] = \"Road Safety Data- Vehicles 2019.csv\" # the original dataset has a small typing mistake\n",
    "FILENAME[\"variable_lookup\"] = \"variable lookup.xls\"\n",
    "\n",
    "DATA_RAW = {}\n",
    "\n",
    "DATA_LEEDS = {}\n",
    "\n",
    "DATA_LEEDS_BIKES = {}\n",
    "\n",
    "VARIABLE_LOOKUP = {}\n",
    "\n",
    "SUMMARY = {}\n",
    "\n",
    "# list of internal names for datasets\n",
    "TABLENAMES = [\"accidents\", \"casualties\", \"vehicles\"]"
   ]
  },
  {
   "source": [
    "## Loading in the Dataset\n",
    "---\n",
    "Starting of the data analysis, we import the given three datasets into a 'pandas' dataframe. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all three datasets using pandas into the predefined dictionary 'DATA_RAW' where the key corresponds to the internal dataset name\n",
    "for name in TABLENAMES:\n",
    "    DATA_RAW[name] = pd.read_csv(PATH['data_raw'] + FILENAME[name])"
   ]
  },
  {
   "source": [
    "## Initial Sanity Check\n",
    "---\n",
    "Before continuing with the data analysis, we want to make sure that the dataset is clean. We do this in a several ways. \n",
    "- (a) Are there multiple indexes in 'accidents.csv'\n",
    "- (b) We check if there are indexes in the sub datasets 'raw_data_vehicles' and 'raw_data_casualties' that are not in the central dataset 'raw_accidents.csv'\n",
    "- (c) We check the missing values in each column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Mulitple Indexes\n",
    "We are first evaluating if there are mulitple indexes in the big dataset 'accidents.csv'. This is a first, very basic metric to determine whether the data in the dataset has been inputted correctly. In this case it is even more important since the accident indexes link the main dataset 'accidents.csv' to the two sub datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# here we check if there are mulitple indexes in the accidents dataset\n",
    "DATA_RAW['accidents'].shape[0] == len(set(DATA_RAW['accidents']['Accident_Index']))"
   ]
  },
  {
   "source": [
    "Perfect. There do not seem to be any multiple indexes. Each row in the dataset 'accidents.csv' seems to refer to a unique accident that we can reference in the two sub datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Wrong Indexes in Sub-Datasets\n",
    "It would be bad if there is information on vehicles and casualties in the two subsets that are referenced by an Accident ID that is not in the main dataset 'accidents.csv'. So we check for those using the below code."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_indexes_in_subset(sub_dataset, _in):\n",
    "    \"\"\" \n",
    "    Helper-Function to evaluate whether there are indexes in the two linked sub datasets that do not appear in the main dataset.\n",
    "\n",
    "    Parameters:\n",
    "        sub_dataset         : pd.DataFrame\n",
    "        _in                 : pd.DataFrame\n",
    "    Return:\n",
    "        #Wrong Indexes      : int (None if len() == 0)\n",
    "    \"\"\"\n",
    "    \n",
    "    accidents_indexes = set(_in['Accident_Index'])\n",
    "    wrong_indexes = [i for i in set(sub_dataset['Accident_Index']) if i not in accidents_indexes]\n",
    "\n",
    "    if len(wrong_indexes) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return len(wrong_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the wrong indexes for each sub dataset\n",
    "for dataset in TABLENAMES:\n",
    "    if dataset != 'accidents':\n",
    "        print(f\"#Missing indexes in {dataset.capitalize()}: {check_indexes_in_subset(DATA_RAW[dataset], DATA_RAW['accidents'])}\")"
   ]
  },
  {
   "source": [
    "This is rather bad. Roughly 21.500 indexes in the 'vehicles.csv' raw dataset are linking to an accident that is not registered in the 'accidents.csv', 19.300 in the 'casualties.csv' are linking casualties to accidents that are not registered int the 'accidents.csv'. If a similar behavior can be observed in the dataset that is filtered for Leeds, we have to think about how to deal with those obvious wrong input of data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Check for missing values\n",
    "Here we check in all columns of all datasets to see if there are missing values (empty string) to get a feeling on which columns we need to further do processing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_columns(data):\n",
    "    \"\"\"\n",
    "    For a dataset provided as a pd.DataFrame the function returns an informative string about each column containing null values,         namely the number of missing values, the column index and the variable name of the column.\n",
    "\n",
    "    Parameters:\n",
    "        data                : pd.DataFrame\n",
    "    Return:\n",
    "        Informative String for each column containing null values, else None\n",
    "    \"\"\"\n",
    "    for column in range(data.shape[1]):\n",
    "        if sum(data.iloc[:,column].isnull()) != 0:\n",
    "            print(f'{sum(data.iloc[:,column].isnull())} ({data.columns[column]}({column}))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    print(dataset.capitalize())\n",
    "    check_all_columns(DATA_RAW[dataset])\n",
    "    print('\\n')"
   ]
  },
  {
   "source": [
    "That's not bad! There are no missing values in both sub datasets. \n",
    "It seems like in the accidents dataset, there are 28 accidents that have no information on their location. This only gets important for our analysis if one of those accidents is located in Leeds, then we would need to deal with this issue later. The LSOA Metric - which is another measure of the accident location of the accident - hasn't been registered for 5714 accidents. This is not important for our analysis, since we will use the longitude and latitude to plot the accidents' location. \n",
    "There are, however, 63 accidents for which the time of accident is not registered. If any of those accidents are located in Leeds, we have to deal with them later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Filtering \n",
    "---\n",
    "Next, we filter the main dataset 'accidents.csv' for the city of interest 'Leeds', which can be identified by several variables in the dataset. We here chose the column 'Local Authority (District)', where 'Leeds' is identified as 204. The resulting, filtered dataframe is saved into a new variable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS[\"accidents\"] = DATA_RAW['accidents'][DATA_RAW['accidents']['Local_Authority_(District)'] == 204]"
   ]
  },
  {
   "source": [
    "However, the other two datasets cannont be identified by the variable attributes, but need to be filtered through the unique accident indexes that we can obtain from our filtered dataframe of accidents in 'Leeds'. We obtain a list of all accident indexes of the accidents that occured in Leeds and use this index list to filter both the 'vehicles.csv' and 'casualties.csv' datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "leeds_indexes = list(DATA_LEEDS['accidents']['Accident_Index']) # we dont need to set() this because we know that all indexes are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS[\"casualties\"] = DATA_RAW['casualties'][DATA_RAW['casualties']['Accident_Index'].isin(leeds_indexes)]\n",
    "DATA_LEEDS[\"vehicles\"] = DATA_RAW['vehicles'][DATA_RAW['vehicles']['Accident_Index'].isin(leeds_indexes)]"
   ]
  },
  {
   "source": [
    "## Saving Filtered Data\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    DATA_LEEDS[dataset].to_csv(PATH['data_interim'] + FILENAME[dataset], index=False)"
   ]
  },
  {
   "source": [
    "## Sanity Check for Leeds\n",
    "--- \n",
    "Now, that we filtered the dataset, we do the exact same sanity checks that we performed on the raw datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Mulitple Indexes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['accidents'].shape[0] == len(set(DATA_LEEDS['accidents']['Accident_Index']))"
   ]
  },
  {
   "source": [
    "### Wrong Indexes in Sub-Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    if dataset != 'accidents':\n",
    "        print(f\"#Missing indexes in {dataset.capitalize()}: {check_indexes_in_subset(DATA_LEEDS[dataset], DATA_LEEDS['accidents'])}\")"
   ]
  },
  {
   "source": [
    "### Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    print(dataset.capitalize())\n",
    "    check_all_columns(DATA_LEEDS[dataset])\n",
    "    print('\\n')"
   ]
  },
  {
   "source": [
    "Perfect. None of the sanity checks reports any problems on our dataset. At this point we could export the dataset and work on the filtered ones. However, we are making some adjustments in the below section to make our analysis easier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Process Data\n",
    "---\n",
    "In this section, the 'Date' and 'Time' attributes in the 'accidents.csv' module will be cleaned for easy use in the single variable analysis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.array(DATA_LEEDS['accidents']['Time'])\n",
    "for i in range(len(time)):\n",
    "    try: \n",
    "        time[i] = time[i][:2]\n",
    "    except:\n",
    "        time[i] = '-1'\n",
    "\n",
    "DATA_LEEDS['accidents']['Time'] = time"
   ]
  },
  {
   "source": [
    "### Date"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Date\n",
    "date = np.array(DATA_LEEDS['accidents']['Date'])\n",
    "\n",
    "\n",
    "for i in range(len(date)):\n",
    "    date[i] = int(date[i][3:5])\n",
    "\n",
    "DATA_LEEDS['accidents']['Date'] = date"
   ]
  },
  {
   "source": [
    "### 2nd Road Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_road_class = np.array(DATA_LEEDS['accidents']['2nd_Road_Class'])\n",
    "\n",
    "for i in range(len(second_road_class)):\n",
    "    if second_road_class[i] == -1:\n",
    "        second_road_class[i] = 0\n",
    "\n",
    "DATA_LEEDS['accidents']['2nd_Road_Class'] = second_road_class"
   ]
  },
  {
   "source": [
    "## Overview \n",
    "---\n",
    "For each of the datasets, we want to get a first good impression of its size and the information it stores. To gain this information, we print out each of the datasets, and get a summary of each of the columns and the uniques."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Accidents\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['accidents'].shape # prints out the number of columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['accidents'] # prints out an overview of the dataframe (and the number of rows and columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['accidents'].nunique() # prints out the column names and the corresponding number of unique values "
   ]
  },
  {
   "source": [
    "We see, that the main dataset 'accidents_processed.csv' stores all recorded accidents in 2019 in Leeds. It consist of 1451 columns (which leads to 1450 recorded accidents) and has 32 columns providing more detailed information about the accident. The different variables and the number of its unique values can be studied in the output of the above cell. We see, that we can differentiate the attributes as follows:\n",
    "- Categorical Attributes (Most of the columns are categorical)\n",
    "- Geographical Attributes (There are several measures of the location of the accident)\n",
    "- Time Attribute (Each accident specifies a date and time)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Vehicles\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['vehicles'].shape # prints out the number of columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['vehicles'] # prints out an overview of the dataframe (and the number of rows and columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['vehicles'].nunique() # prints out the column names and the corresponding number of unique values "
   ]
  },
  {
   "source": [
    "We see, that the side dataset 'vehicles_processed.csv' provides more detailed information about all vehicles involved in each of the accidents. It consist of 2688 columns (which leads to 2688 records on involved vehicles) and has 23 columns providing more detailed information about the vehicle. The different variables and the number of its unique values can be studied in the output of the above cell. We see, that we can differentiate the attributes as follows:\n",
    "- Linking Attributes (Accident Indexes link the vehicles to the accidents dataset and the vehicle references the casualties)\n",
    "- Categorical Attributes (Most of the columns are categorical)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Casualties\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['casualties'].shape # prints out the number of columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['casualties'] # prints out an overview of the dataframe (and the number of rows and columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['casualties'].nunique() # prints out the column names and the corresponding number of unique values "
   ]
  },
  {
   "source": [
    "We see, that the side dataset 'casualties_processed.csv' provides more detailed information about the casualties of all lethal accidents. It consist of 1908 columns (which leads to 1907 records on casualties) and has 16 columns providing more detailed information about the vehicle. The different variables and the number of its unique values can be studied in the output of the above cell. We see, that we can differentiate the attributes as follows:\n",
    "- Linking Attributes (Accident Indexes link the vehicles to the accidents dataset and the vehicle references the casualties)\n",
    "- Categorical Attributes (Most of the columns are categorical)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Export Processed Datasets\n",
    "--- \n",
    "Finally, we export the processed datasets into a new subfolder. From now on, all Jupyter Notebooks will work with those processed datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    DATA_LEEDS[dataset].to_csv(PATH['data_processed'] + FILENAME[dataset], index=False)"
   ]
  },
  {
   "source": [
    "# Single Variable Analysis\n",
    "---\n",
    "### Notebook for Single Variable Analysis in all three processed datasets\n",
    "\n",
    "Creation: 07.02.2021"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing Variable Lookup \n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile(PATH['references'] + FILENAME['variable_lookup'])\n",
    "\n",
    "# read in excel data into python dict of dicts\n",
    "excel_dict = {i: xls.parse(xls.sheet_names[i]).to_dict() for i in range(len(xls.sheet_names))}\n",
    "\n",
    "# create convenient lookup dictionary for all excel sheets\n",
    "for x in range(2, len(excel_dict)):\n",
    "    VARIABLE_LOOKUP[x] = {}\n",
    "    for title in [('code', 'label'), ('Code', 'Label')]:\n",
    "        try:\n",
    "            for i in range(len(excel_dict[x][title[0]])):\n",
    "                VARIABLE_LOOKUP[x][excel_dict[x][title[0]][i]] = excel_dict[x][title[1]][i]\n",
    "            continue\n",
    "        except: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del VARIABLE_LOOKUP[36][' M']\n",
    "VARIABLE_LOOKUP[36][-1] = 'Undefined'"
   ]
  },
  {
   "source": [
    "## Create Summary Dictionary\n",
    "---\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {}\n",
    "LABELS['accidents'] = [5, 6, 10, 12, 13, 14, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
    "LABELS['casualties'] = [3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "LABELS['vehicles'] = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 18]\n",
    "\n",
    "PLOTTING = {}\n",
    "PLOTTING['accidents'] = [None, None, None, None, None, 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', None]\n",
    "PLOTTING['casualties'] = [None, None, None, 'bar', 'bar', 'hist', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar',None]\n",
    "PLOTTING['vehicles'] = [None, None, 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'hist', 'bar', 'hist', 'bar', 'hist', None, 'bar', None]\n",
    "\n",
    "FIVENUM = {}\n",
    "FIVENUM['accidents'] = [False, False, False, False, False, False, True, True, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
    "FIVENUM['casualties'] = [False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False]\n",
    "FIVENUM['vehicles'] = [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, True, False, False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_summary(dataset, labels, plotting, fivenum, start_at=0):\n",
    "    # initialise the lookup dictionary with the column name and variable type\n",
    "    SUMMARY[dataset] = {}\n",
    "    for i in range(DATA_LEEDS[dataset].shape[1]):\n",
    "        SUMMARY[dataset][i] = {'Name': list(DATA_LEEDS[dataset])[i]}\n",
    "        if plotting[i] == 'bar':\n",
    "            SUMMARY[dataset][i].update({'Plot': 'bar'})\n",
    "        elif plotting[i] == 'hist':\n",
    "            SUMMARY[dataset][i].update({'Plot': 'hist'})\n",
    "        else: SUMMARY[dataset][i].update({'Plot': None})\n",
    "\n",
    "        if fivenum[i] == True:\n",
    "            SUMMARY[dataset][i].update({'Summary': True})\n",
    "        else: SUMMARY[dataset][i].update({'Summary': False})\n",
    "\n",
    "    # add the maps to the lookup dictionary\n",
    "    categorical_counter = 0\n",
    "    for column in labels:\n",
    "        if dataset == 'casualties':\n",
    "            if categorical_counter == 2: \n",
    "                SUMMARY[dataset][column]['Map'] = VARIABLE_LOOKUP[35]\n",
    "                categorical_counter += 1\n",
    "                continue\n",
    "            if categorical_counter == 10:\n",
    "                SUMMARY[dataset][column]['Map'] = VARIABLE_LOOKUP[48]\n",
    "                categorical_counter += 1\n",
    "                continue\n",
    "            if categorical_counter == 11:\n",
    "                SUMMARY[dataset][column]['Map'] = VARIABLE_LOOKUP[47]\n",
    "                categorical_counter += 1\n",
    "                continue\n",
    "        \n",
    "        SUMMARY[dataset][column]['Map'] = VARIABLE_LOOKUP[start_at+categorical_counter]\n",
    "        categorical_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, start_at in zip(list(range(3)), [2, 37, 22]):\n",
    "    initialise_summary(TABLENAMES[i], LABELS[TABLENAMES[i]], PLOTTING[TABLENAMES[i]], FIVENUM[TABLENAMES[i]], start_at=start_at)"
   ]
  },
  {
   "source": [
    "We also want to have a nice mapping for the time and date. We do this manually, since it is not defined in the given variable lookup excel."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time mapping\n",
    "SUMMARY['accidents'][11]['Map'] = {i: f\"{i}-{i+1}\" for i in range(24)}\n",
    "\n",
    "# date mapping\n",
    "months = ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']\n",
    "SUMMARY['accidents'][9]['Map'] = {i+1: months[i] for i in range(12)}"
   ]
  },
  {
   "source": [
    "## Numerical Analysis\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniques_and_counts(data):\n",
    "    uniques, counts = np.unique(data, return_counts=True)\n",
    "\n",
    "    return uniques, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fivenumsummary(data):\n",
    "    return np.percentile(data[data >= 0], [0, 25, 50, 75, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary_categorical(summary, data): #\n",
    "    for column in range(len(summary)):\n",
    "        uniques, counts = get_uniques_and_counts(data[summary[column]['Name']])\n",
    "        summary[column]['No_Uniques'] = len(uniques) # get the number of uniques for each variable\n",
    "            \n",
    "        if summary[column]['Plot'] == 'bar': # get the counts for each uniques for all categorical attributes\n",
    "            if len(uniques) < 100:\n",
    "                summary[column]['Uniques'] = {uniques[i]: counts[i] for i in range(len(uniques))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary_numerical(summary, data):\n",
    "    for column in range(len(summary)):\n",
    "        if summary[column]['Summary'] == True:\n",
    "            summary[column]['Five_Number_Summary'] = get_fivenumsummary(data[summary[column]['Name']])\n",
    "        if summary[column]['Plot'] == 'hist' or summary[column]['Summary'] == True:\n",
    "            summary[column]['Data'] = np.array(data[summary[column]['Name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    compute_summary_categorical(SUMMARY[dataset], DATA_LEEDS[dataset])\n",
    "    compute_summary_numerical(SUMMARY[dataset], DATA_LEEDS[dataset])"
   ]
  },
  {
   "source": [
    "## Saving Numerical Reports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_numerical_report(summary, name, save_to='csv'):\n",
    "    summary_dataframe = pd.DataFrame(summary)\n",
    "\n",
    "    if save_to == 'csv':\n",
    "\n",
    "        try: os.makedirs('../reports/numerical_summaries/csv')\n",
    "        except: None\n",
    "        \n",
    "        summary_dataframe.to_csv(f'../reports/numerical_summaries/csv/numerical_summary_{name}.csv')\n",
    "    elif save_to == 'json':\n",
    "        \n",
    "        try: os.makedirs('../reports/numerical_summaries/json')\n",
    "        except: None\n",
    "\n",
    "        summary_dataframe.to_json(f'../reports/numerical_summaries/json/numerical_summary_{name}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(TABLENAMES)):\n",
    "    save_numerical_report(SUMMARY[TABLENAMES[i]], TABLENAMES[i], save_to='csv')\n",
    "    save_numerical_report(SUMMARY[TABLENAMES[i]], TABLENAMES[i], save_to='json')"
   ]
  },
  {
   "source": [
    "## Visualisation\n",
    "---\n",
    "In this section, we are visualising the results of the single variable analysis in all three datasets - both within the Jupyter and per export into 'reports/figures' for later use."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot(data, keep_missing_values=True):\n",
    "    # create figure and axes (with padding for better exporting)\n",
    "    fig = plt.figure(figsize=(32,18))\n",
    "    ax = fig.add_axes([.15,.15,.7,.7])\n",
    "    \n",
    "    # variables depending on missing_values variable\n",
    "    if keep_missing_values:\n",
    "        x = list(data['Uniques'].keys())\n",
    "        y = list(data['Uniques'].values())\n",
    "        title = title = f\"Distribution: {data['Name'].replace('_', ' ')} (with missing values)\"\n",
    "        color = 'darkred'\n",
    "        yticks = list(data['Uniques'].keys())\n",
    "    \n",
    "    else: \n",
    "        if -1 in list(data['Uniques'].keys()):\n",
    "            x = list(data['Uniques'].keys())[1:]\n",
    "            y = list(data['Uniques'].values())[1:]\n",
    "            yticks = list(data['Uniques'].keys())[1:]\n",
    "        else: \n",
    "            x = list(data['Uniques'].keys())\n",
    "            y = list(data['Uniques'].values())\n",
    "            yticks = list(data['Uniques'].keys())\n",
    "\n",
    "        title = f\"Distribution: {data['Name'].replace('_', ' ')} (without missing values)\"\n",
    "        color = 'darkblue'\n",
    "        \n",
    "    spaced_ticks = [i for i in range(len(yticks))]\n",
    "    # plot bar \n",
    "    ax.barh(spaced_ticks, y, align='center', color=color)\n",
    "\n",
    "    # set title\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "\n",
    "    # format axis-labels\n",
    "    ax.set_xlabel('Number of Accidents')\n",
    "    try: # account for 0% datasets\n",
    "        ax.set_xlim(0, 1.15*max(y)) \n",
    "    except: None\n",
    "\n",
    "    ax.set_yticks(spaced_ticks)\n",
    "    ax.tick_params(axis='y', which='major', pad=10)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    try: \n",
    "        y_labels = [data['Map'][i] for i in x] # use lookup from xls \n",
    "        ax.set_yticklabels([textwrap.fill(label, 10) for label in y_labels])\n",
    "    except: None # account for variables that do not have lookup mapping\n",
    "\n",
    "    # insert counts and percentages as text next to the corresponding bars\n",
    "    for x_cord, y_cord in zip(spaced_ticks,y):\n",
    "        ax.text(y_cord, x_cord, f'{y_cord} ({str(100*round(y_cord/sum(y), 3))[:5]}%)' , color='black', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(data, keep_missing_values=True):\n",
    "    # create figure and axes (with padding for better exporting)\n",
    "    fig = plt.figure(figsize=(16,9))\n",
    "    ax = fig.add_axes([.1,.1,.8,.8])\n",
    "\n",
    "    if keep_missing_values:\n",
    "        title = f\"Distribution: {data['Name'].replace('_', ' ')} (with missing values)\"\n",
    "        data = data['Data']\n",
    "        color = 'darkred'\n",
    "\n",
    "    else: \n",
    "        title = f\"Distribution: {data['Name'].replace('_', ' ')} (without missing values)\"\n",
    "        data = data['Data'][(data['Data'] != -1)] # masking out -1\n",
    "        color = 'darkblue'\n",
    "\n",
    "    ax.hist(data, bins=50, color=color)\n",
    "\n",
    "    # set title\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "\n",
    "    # format axis-labels\n",
    "    ax.set_ylabel('Number of Accidents')\n",
    "    ax.set_xlabel('Age')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot(data):\n",
    "    # create figure and axes (with padding for better exporting)\n",
    "    fig = plt.figure(figsize=(16,9))\n",
    "    ax = fig.add_axes([.1,.1,.8,.8])\n",
    "\n",
    "    ax.boxplot(data['Data']);\n",
    "\n",
    "    ax.set_title(f\"Boxplot of {data['Name'].replace('_', ' ')}\", fontweight='bold')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "source": [
    "## Saving Figures\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figure(figure, path, column_index, column_name, dataset_name, _type='pdf'):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except: None\n",
    "\n",
    "    figure.savefig(f'{path}/{column_index}_{column_name}.{_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_sva(summary, dataset_name, missing_values):\n",
    "    for column in range(len(summary)):\n",
    "        if summary[column]['Plot'] == 'bar':\n",
    "            # create barplot\n",
    "            fig = barplot(summary[column], keep_missing_values=missing_values)\n",
    "\n",
    "            # set path to save to \n",
    "            if missing_values: path = PATH['reports'][dataset_name] + 'single_variable_analysis/' + 'with_missing_values'\n",
    "            else: path = PATH[\"reports\"][dataset_name] + 'single_variable_analysis/' + 'without_missing_values'\n",
    "\n",
    "            # save to path\n",
    "            save_figure(fig, path, column_index=column, column_name=summary[column]['Name'], dataset_name=dataset_name, _type='pdf')\n",
    "\n",
    "        elif summary[column]['Plot'] == 'hist':\n",
    "            # create histogram\n",
    "            fig = histogram(summary[column], keep_missing_values=missing_values)\n",
    "\n",
    "            # set path to save to \n",
    "            if missing_values: path = PATH['reports'][dataset_name] + 'single_variable_analysis/' + 'with_missing_values'\n",
    "            else: path = PATH[\"reports\"][dataset_name] + 'single_variable_analysis/' + 'without_missing_values'\n",
    "\n",
    "            save_figure(fig, path, column_index=column, column_name=summary[column]['Name'], dataset_name=dataset_name, _type='pdf')\n",
    "\n",
    "        if summary[column]['Summary']:\n",
    "            # create boxplot\n",
    "            fig = boxplot(summary[column])\n",
    "\n",
    "            # set path to save to \n",
    "            path = PATH['reports'][dataset_name] + 'single_variable_analysis/' + 'boxplots'\n",
    "            \n",
    "            save_figure(fig, path, column_index=column, column_name=summary[column]['Name'], dataset_name=dataset_name, _type='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "for dataset in TABLENAMES:\n",
    "    save_all_sva(SUMMARY[dataset], dataset, missing_values=True)\n",
    "    save_all_sva(SUMMARY[dataset], dataset, missing_values=False)"
   ]
  },
  {
   "source": [
    "# Associations (TASK 2)\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Categorical/ Numerical Analysis (Swarm Plots)\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_scatterplot(data, summary_categorical_variable, summary_numerical_variable, kind='svarm'):\n",
    "    name_categorical = summary_categorical_variable['Name']\n",
    "    data_categorical = data[name_categorical]\n",
    "    \n",
    "    name_numerical = summary_numerical_variable['Name']\n",
    "    data_numerical = data[name_numerical]\n",
    "\n",
    "    data_to_plot = np.array([data_categorical, data_numerical]).T\n",
    "\n",
    "    if kind == 'svarm':\n",
    "        fig = sns.catplot(x = name_categorical, y = name_numerical, data=pd.DataFrame(data_to_plot, columns=[name_categorical, name_numerical]),    kind='swarm', height=8.27, aspect=11.7/8.27, legend=True)\n",
    "    elif kind == 'violin':\n",
    "        pass\n",
    "    else: raise NameError(f\"type = '{kind}'' is not defined. Try 'svarm' or 'violin'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_association_test():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_scatterplot(DATA_LEEDS['casualties'], SUMMARY['casualties'][13], SUMMARY['casualties'][5], kind='svarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(SUMMARY['casualties'])):\n",
    "    if SUMMARY['casualties'][i]['Plot'] == 'hist':\n",
    "        for j in range(len(SUMMARY['casualties'])):\n",
    "            if if SUMMARY['casualties'][j]['Plot'] == 'bar':\n",
    "                categorical_scatterplot(DATA_LEEDS['casualties'], SUMMARY['casualties'][i], SUMMARY['casualties'][j]), kind='svarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter\n",
    "mask = (dataraw[\"casualties\"][\"Casualty_Type\"] == 0) | (dataraw[\"casualties\"][\"Casualty_Type\"] == 1) | (dataraw[\"casualties\"][\"Casualty_Type\"] == 9) &    (dataraw[\"casualties\"][\"Age_of_Casualty\"] > -1)\n",
    "mask[600:] = False # Restrict to at most 600 points\n",
    "\n",
    "data_toplot = np.array([dataraw[\"casualties\"][\"Casualty_Type\"][mask], dataraw[\"casualties\"][\"Age_of_Casualty\"][mask]]).T\n",
    "\n",
    "# Plot\n",
    "fig = sns.catplot(x = 'Casualty Type', y = 'Age of Casualty', data = pd.DataFrame(data_toplot, columns=['Casualty Type', 'Age of Casualty']), kind = \"swarm\") # also show: violin\n",
    "fig.set_xticklabels([\"Pedestrian\", \"Cyclist\", \"Car occupant\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Categorical/ Categorical Analysis"
   ]
  },
  {
   "source": [
    "# Spatial Visualisation (TASK 3)\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Create Map of Leeds\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leeds_map = folium.Map(location=[53.8008, -1.5491], tiles=\"Stamen Terrain\", initial_zoom = 5)\n",
    "leeds_map"
   ]
  },
  {
   "source": [
    "## Plot all Accidents in Leeds onto Map\n",
    "--- "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_point(map, location, color, fill):\n",
    "    folium.Circle(\n",
    "            radius=30,\n",
    "            location=[location[0], location[1]],\n",
    "            color=color,\n",
    "            fill=True).add_to(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(DATA_LEEDS['accidents']['Accident_Severity'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_accidents(data, focus='Accident_Severity'):\n",
    "    uniques = np.unique(data[focus])\n",
    "    no_uniques = len(uniques)\n",
    "    # colors = [list(np.random.choice(range(256), size=3)) for _ in range(no_uniques)]\n",
    "    colors = ['black', 'red', 'green']\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(no_uniques):\n",
    "            if data[focus].iloc[i] == uniques[j]:\n",
    "                plot_point(leeds_map, (data['Latitude'].iloc[i], data['Longitude'].iloc[i]), color=colors[j], fill=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_accidents(DATA_LEEDS['accidents'], focus='Accident_Severity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leeds_map"
   ]
  },
  {
   "source": [
    "## Save Map Visualisation\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_map(map_, index, name):\n",
    "    try:\n",
    "        os.makedirs(f'../reports/maps')\n",
    "    except: None\n",
    "\n",
    "    map_.save(f'../reports/maps/{index}_{name}_map.html')"
   ]
  },
  {
   "source": [
    "# Bike Safety in Leeds (TASK 4)\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Filtering for Bike Accidents\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS_BIKES['vehicles'] = DATA_LEEDS['vehicles'][DATA_LEEDS['vehicles']['Vehicle_Type'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_accidents_indexes = set(DATA_LEEDS_BIKES['vehicles']['Accident_Index'])\n",
    "len(bike_accidents_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    if dataset != 'vehicles':\n",
    "        DATA_LEEDS_BIKES[dataset] = DATA_LEEDS[dataset][DATA_LEEDS[dataset]['Accident_Index'].isin(bike_accidents_indexes)]"
   ]
  }
 ]
}