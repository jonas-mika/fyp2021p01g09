{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Final Report for Leeds Road Safety Plan for 2021\n",
    "---\n",
    "Date: 25.02.2021\n",
    "Group 9: \n",
    "\n",
    "Experience shows that informed policy-making often translates into good policy-making. Today we explore another important element of road safety outlined in the Best Practice Guide of the Safety Cycling Advocacy Program: data collection. Getting a comprehensive view of the risks that cyclists face, will enable urban planners to design better solutions for cycling safety.\n",
    "\n",
    "Leeds is the largest city in the county of West Yorkshire, England and the most populous in the Yorkshire and Humber region.\n",
    "Leeds: 792,525 (2019) ()\n",
    "\n",
    "Leeds' transport system is dominated by cars and it is currently the 9th most congested UK city, with congestion costing £1,057 per driver.[132]\n",
    "\n",
    "There have long been plans for a public transport network in Leeds, yet as of 2020, none have come to fruition. In the 1940s, there were plans to build an extensive underground system, however these were scuppered by the Second World War.[133] There were plans for a Leeds Supertram in the 1990s and £500 million in funding was to be provided; however, due to spiralling costs, the plans were cancelled by the Transport Minister Alistair Darling in 2005, even though £40 million had already been spent on the project. Hopes were renewed with the proposal for a £250 million New Generation Transport Trolleybus service in 2007; however, after a long wait and millions of pounds spent on inquiries, the plans were cancelled in May 2016 citing little value for money.[134]\n",
    "\n",
    "In June 2019, as part of his bid to become Prime Minister, Boris Johnson stated that it was \"madness\" that Leeds did not have a metro system.[135] In December 2019, during his first Queen's Speech, Johnson promised to \"remedy the scandal that Leeds is the largest city in Western Europe without light rail or a metro\"."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Required Libraries\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                         # used to store the datasets\n",
    "import numpy as np                          # used for numerical calculations on individual column\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt             # plotting \n",
    "import seaborn as sns                       # plotting numerical against categorical variables\n",
    "import textwrap                             # prettify axis labelling\n",
    "import json                                 # data transfer to json format\n",
    "import os                                   # automates saving of export files (figures, numerical summaries, ...)\n",
    "import folium                               # spatial visualisation\n",
    "from scipy.stats import chi2_contingency    # chi2 statistical association test"
   ]
  },
  {
   "source": [
    "## Constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path lookup dictionary to store the relative paths from the directory containing the jupyter notebooks to important directories in the project\n",
    "PATH = {}\n",
    "PATH[\"data_raw\"] = \"../data/raw/\"\n",
    "PATH[\"data_interim\"] = \"../data/interim/\"\n",
    "PATH[\"data_processed\"] = \"../data/processed/\"\n",
    "PATH[\"data_external\"] = \"../data/external/\"\n",
    "PATH[\"references\"] = \"../data/references/\"\n",
    "PATH['reports'] = {}\n",
    "PATH['reports']['accidents'] = '../reports/accidents/'\n",
    "PATH['reports']['casualties'] = '../reports/casualties/'\n",
    "PATH['reports']['vehicles'] = '../reports/vehicles/'\n",
    "PATH['reports']['external'] = '../reports/external/'\n",
    "\n",
    "\n",
    "# filename lookup dictionary storing the most relevant filenames\n",
    "FILENAME = {}\n",
    "FILENAME['accidents'] = 'Road Safety Data - Accidents 2019.csv'\n",
    "FILENAME['casualties'] = 'Road Safety Data - Casualties 2019.csv'\n",
    "FILENAME['vehicles'] = 'Road Safety Data- Vehicles 2019.csv' # the original dataset has a small typing mistake\n",
    "FILENAME['variable_lookup'] = 'variable lookup.xls'\n",
    "FILENAME['road_flow'] = 'dft_rawcount_local_authority_id_63.csv'\n",
    "\n",
    "DATA_RAW = {}\n",
    "\n",
    "DATA_LEEDS = {}\n",
    "\n",
    "EXTERNAL = {}\n",
    "\n",
    "VARIABLE_LOOKUP = {}\n",
    "\n",
    "SUMMARY = {}\n",
    "\n",
    "DATA_LEEDS_BIKES = {}\n",
    "\n",
    "SUMMARY_BIKES = {}\n",
    "\n",
    "# list of internal names for datasets\n",
    "TABLENAMES = [\"accidents\", \"casualties\", \"vehicles\"]"
   ]
  },
  {
   "source": [
    "# Loading, Inspection and Processing of Datasets (TASK 0)\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Loading in the Dataset\n",
    "---\n",
    "Starting of the data analysis, we import the given three datasets into a 'pandas' dataframe. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all three datasets using pandas into the predefined dictionary 'DATA_RAW' where the key corresponds to the internal dataset name\n",
    "for name in TABLENAMES:\n",
    "    DATA_RAW[name] = pd.read_csv(PATH['data_raw'] + FILENAME[name])"
   ]
  },
  {
   "source": [
    "## Initial Sanity Check\n",
    "---\n",
    "Before continuing with the data analysis, we want to make sure that the dataset is clean. We do this in a several ways. \n",
    "- (a) Are there multiple indexes in 'accidents.csv'\n",
    "- (b) We check if there are indexes in the sub datasets 'raw_data_vehicles' and 'raw_data_casualties' that are not in the central dataset 'raw_accidents.csv'\n",
    "- (c) We check the missing values in each column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Mulitple Indexes\n",
    "We are first evaluating if there are mulitple indexes in the big dataset 'accidents.csv'. This is a first, very basic metric to determine whether the data in the dataset has been inputted correctly. In this case it is even more important since the accident indexes link the main dataset 'accidents.csv' to the two sub datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we check if there are mulitple indexes in the accidents dataset\n",
    "DATA_RAW['accidents'].shape[0] == len(set(DATA_RAW['accidents']['Accident_Index']))"
   ]
  },
  {
   "source": [
    "Perfect. There do not seem to be any multiple indexes. Each row in the dataset 'accidents.csv' seems to refer to a unique accident that we can reference in the two sub datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Wrong Indexes in Sub-Datasets\n",
    "It would be bad if there is information on vehicles and casualties in the two subsets that are referenced by an Accident ID that is not in the main dataset 'accidents.csv'. So we check for those using the below code."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_indexes_in_subset(sub_dataset, _in):\n",
    "    \"\"\" \n",
    "    Helper-Function to evaluate whether there are indexes in the two linked sub datasets that do not appear in the main dataset.\n",
    "\n",
    "    Parameters:\n",
    "        sub_dataset         : pd.DataFrame\n",
    "        _in                 : pd.DataFrame\n",
    "    Return:\n",
    "        #Wrong Indexes      : int (None if len() == 0)\n",
    "    \"\"\"\n",
    "    \n",
    "    accidents_indexes = set(_in['Accident_Index'])\n",
    "    wrong_indexes = [i for i in set(sub_dataset['Accident_Index']) if i not in accidents_indexes]\n",
    "\n",
    "    if len(wrong_indexes) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return len(wrong_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the wrong indexes for each sub dataset\n",
    "for dataset in TABLENAMES:\n",
    "    if dataset != 'accidents':\n",
    "        print(f\"#Missing indexes in {dataset.capitalize()}: {check_indexes_in_subset(DATA_RAW[dataset], DATA_RAW['accidents'])}\")"
   ]
  },
  {
   "source": [
    "This is rather bad. Roughly 21.500 indexes in the 'vehicles.csv' raw dataset are linking to an accident that is not registered in the 'accidents.csv', 19.300 in the 'casualties.csv' are linking casualties to accidents that are not registered int the 'accidents.csv'. If a similar behavior can be observed in the dataset that is filtered for Leeds, we have to think about how to deal with those obvious wrong input of data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Check for missing values\n",
    "Here we check in all columns of all datasets to see if there are missing values (empty string) to get a feeling on which columns we need to further do processing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_columns(data):\n",
    "    \"\"\"\n",
    "    For a dataset provided as a pd.DataFrame the function returns an informative string about each column containing null values,         namely the number of missing values, the column index and the variable name of the column.\n",
    "\n",
    "    Parameters:\n",
    "        data                : pd.DataFrame\n",
    "    Return:\n",
    "        Informative String for each column containing null values, else None\n",
    "    \"\"\"\n",
    "    for column in range(data.shape[1]):\n",
    "        if sum(data.iloc[:,column].isnull()) != 0:\n",
    "            print(f'{sum(data.iloc[:,column].isnull())} ({data.columns[column]}({column}))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    print(dataset.capitalize())\n",
    "    check_all_columns(DATA_RAW[dataset])\n",
    "    print('\\n')"
   ]
  },
  {
   "source": [
    "That's not bad! There are no missing values in both sub datasets. \n",
    "It seems like in the accidents dataset, there are 28 accidents that have no information on their location. This only gets important for our analysis if one of those accidents is located in Leeds, then we would need to deal with this issue later. The LSOA Metric - which is another measure of the accident location of the accident - hasn't been registered for 5714 accidents. This is not important for our analysis, since we will use the longitude and latitude to plot the accidents' location. \n",
    "There are, however, 63 accidents for which the time of accident is not registered. If any of those accidents are located in Leeds, we have to deal with them later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Filtering \n",
    "---\n",
    "Next, we filter the main dataset 'accidents.csv' for the city of interest 'Leeds', which can be identified by several variables in the dataset. We here chose the column 'Local Authority (District)', where 'Leeds' is identified as 204. The resulting, filtered dataframe is saved into a new variable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS[\"accidents\"] = DATA_RAW['accidents'][DATA_RAW['accidents']['Local_Authority_(District)'] == 204]"
   ]
  },
  {
   "source": [
    "However, the other two datasets cannont be identified by the variable attributes, but need to be filtered through the unique accident indexes that we can obtain from our filtered dataframe of accidents in 'Leeds'. We obtain a list of all accident indexes of the accidents that occured in Leeds and use this index list to filter both the 'vehicles.csv' and 'casualties.csv' datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "leeds_indexes = list(DATA_LEEDS['accidents']['Accident_Index']) # we dont need to set() this because we know that all indexes are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS[\"casualties\"] = DATA_RAW['casualties'][DATA_RAW['casualties']['Accident_Index'].isin(leeds_indexes)]\n",
    "DATA_LEEDS[\"vehicles\"] = DATA_RAW['vehicles'][DATA_RAW['vehicles']['Accident_Index'].isin(leeds_indexes)]"
   ]
  },
  {
   "source": [
    "## Saving Filtered Data\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    DATA_LEEDS[dataset].to_csv(PATH['data_interim'] + FILENAME[dataset], index=False)"
   ]
  },
  {
   "source": [
    "## Sanity Check for Leeds\n",
    "--- \n",
    "Now, that we filtered the dataset, we do the exact same sanity checks that we performed on the raw datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Mulitple Indexes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['accidents'].shape[0] == len(set(DATA_LEEDS['accidents']['Accident_Index']))"
   ]
  },
  {
   "source": [
    "### Wrong Indexes in Sub-Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    if dataset != 'accidents':\n",
    "        print(f\"#Missing indexes in {dataset.capitalize()}: {check_indexes_in_subset(DATA_LEEDS[dataset], DATA_LEEDS['accidents'])}\")"
   ]
  },
  {
   "source": [
    "### Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    print(dataset.capitalize())\n",
    "    check_all_columns(DATA_LEEDS[dataset])\n",
    "    print('\\n')"
   ]
  },
  {
   "source": [
    "Perfect. None of the sanity checks reports any problems on our dataset. At this point we could export the dataset and work on the filtered ones. However, we are making some adjustments in the below section to make our analysis easier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Process Data\n",
    "---\n",
    "In this section, the 'Date' and 'Time' attributes in the 'accidents.csv' module will be cleaned for easy use in the single variable analysis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.array(DATA_LEEDS['accidents']['Time'])\n",
    "for i in range(len(time)):\n",
    "    try: \n",
    "        time[i] = time[i][:2]\n",
    "    except:\n",
    "        time[i] = '-1'\n",
    "\n",
    "DATA_LEEDS['accidents']['Time'] = time"
   ]
  },
  {
   "source": [
    "### Date"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Date\n",
    "date = np.array(DATA_LEEDS['accidents']['Date'])\n",
    "\n",
    "\n",
    "for i in range(len(date)):\n",
    "    date[i] = int(date[i][3:5])\n",
    "\n",
    "DATA_LEEDS['accidents']['Date'] = date"
   ]
  },
  {
   "source": [
    "### 2nd Road Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_road_class = np.array(DATA_LEEDS['accidents']['2nd_Road_Class'])\n",
    "\n",
    "for i in range(len(second_road_class)):\n",
    "    if second_road_class[i] == -1:\n",
    "        second_road_class[i] = 0\n",
    "\n",
    "DATA_LEEDS['accidents']['2nd_Road_Class'] = second_road_class"
   ]
  },
  {
   "source": [
    "## Overview \n",
    "---\n",
    "For each of the datasets, we want to get a first good impression of its size and the information it stores. To gain this information, we print out each of the datasets, and get a summary of each of the columns and the uniques."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Accidents\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['accidents'].shape # prints out the number of columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['accidents'] # prints out an overview of the dataframe (and the number of rows and columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['accidents'].nunique() # prints out the column names and the corresponding number of unique values "
   ]
  },
  {
   "source": [
    "We see, that the main dataset 'accidents_processed.csv' stores all recorded accidents in 2019 in Leeds. It consist of 1451 columns (which leads to 1450 recorded accidents) and has 32 columns providing more detailed information about the accident. The different variables and the number of its unique values can be studied in the output of the above cell. We see, that we can differentiate the attributes as follows:\n",
    "- Categorical Attributes (Most of the columns are categorical)\n",
    "- Geographical Attributes (There are several measures of the location of the accident)\n",
    "- Time Attribute (Each accident specifies a date and time)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Vehicles\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['vehicles'].shape # prints out the number of columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['vehicles'] # prints out an overview of the dataframe (and the number of rows and columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['vehicles'].nunique() # prints out the column names and the corresponding number of unique values "
   ]
  },
  {
   "source": [
    "We see, that the side dataset 'vehicles_processed.csv' provides more detailed information about all vehicles involved in each of the accidents. It consist of 2688 columns (which leads to 2688 records on involved vehicles) and has 23 columns providing more detailed information about the vehicle. The different variables and the number of its unique values can be studied in the output of the above cell. We see, that we can differentiate the attributes as follows:\n",
    "- Linking Attributes (Accident Indexes link the vehicles to the accidents dataset and the vehicle references the casualties)\n",
    "- Categorical Attributes (Most of the columns are categorical)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Casualties\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['casualties'].shape # prints out the number of columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['casualties'] # prints out an overview of the dataframe (and the number of rows and columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['casualties'].nunique() # prints out the column names and the corresponding number of unique values "
   ]
  },
  {
   "source": [
    "We see, that the side dataset 'casualties_processed.csv' provides more detailed information about the casualties of all lethal accidents. It consist of 1908 columns (which leads to 1907 records on casualties) and has 16 columns providing more detailed information about the vehicle. The different variables and the number of its unique values can be studied in the output of the above cell. We see, that we can differentiate the attributes as follows:\n",
    "- Linking Attributes (Accident Indexes link the vehicles to the accidents dataset and the vehicle references the casualties)\n",
    "- Categorical Attributes (Most of the columns are categorical)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Export Processed Datasets\n",
    "--- \n",
    "Finally, we export the processed datasets into a new subfolder. From now on, all Jupyter Notebooks will work with those processed datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    DATA_LEEDS[dataset].to_csv(PATH['data_processed'] + FILENAME[dataset], index=False)"
   ]
  },
  {
   "source": [
    "# Single Variable Analysis (TASK 1)\n",
    "---\n",
    "### Notebook for Single Variable Analysis in all three processed datasets\n",
    "\n",
    "Creation: 07.02.2021"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing Variable Lookup \n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile(PATH['references'] + FILENAME['variable_lookup'])\n",
    "\n",
    "# read in excel data into python dict of dicts\n",
    "excel_dict = {i: xls.parse(xls.sheet_names[i]).to_dict() for i in range(len(xls.sheet_names))}\n",
    "\n",
    "# create convenient lookup dictionary for all excel sheets\n",
    "for x in range(2, len(excel_dict)):\n",
    "    VARIABLE_LOOKUP[x] = {}\n",
    "    for title in [('code', 'label'), ('Code', 'Label')]:\n",
    "        try:\n",
    "            for i in range(len(excel_dict[x][title[0]])):\n",
    "                VARIABLE_LOOKUP[x][excel_dict[x][title[0]][i]] = excel_dict[x][title[1]][i]\n",
    "            continue\n",
    "        except: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del VARIABLE_LOOKUP[36][' M']\n",
    "VARIABLE_LOOKUP[36][-1] = 'Undefined'"
   ]
  },
  {
   "source": [
    "## Create Summary Dictionary\n",
    "---\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {}\n",
    "LABELS['accidents'] = [5, 6, 10, 12, 13, 14, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
    "LABELS['casualties'] = [3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "LABELS['vehicles'] = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 18]\n",
    "\n",
    "PLOTTING = {}\n",
    "PLOTTING['accidents'] = [None, None, None, None, None, 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', None]\n",
    "PLOTTING['casualties'] = [None, None, None, 'bar', 'bar', 'hist', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar',None]\n",
    "PLOTTING['vehicles'] = [None, None, 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'bar', 'hist', 'bar', 'hist', 'bar', 'hist', None, 'bar', None]\n",
    "\n",
    "FIVENUM = {}\n",
    "FIVENUM['accidents'] = [False, False, False, False, False, False, True, True, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
    "FIVENUM['casualties'] = [False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False]\n",
    "FIVENUM['vehicles'] = [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, True, False, False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_summary(dataset, summary,labels, plotting, fivenum, start_at=0):\n",
    "    # initialise the lookup dictionary with the column name and variable type\n",
    "    summary[dataset] = {}\n",
    "    for i in range(DATA_LEEDS[dataset].shape[1]):\n",
    "        summary[dataset][i] = {'Name': list(DATA_LEEDS[dataset])[i]}\n",
    "        if plotting[i] == 'bar':\n",
    "            summary[dataset][i].update({'Plot': 'bar'})\n",
    "        elif plotting[i] == 'hist':\n",
    "            summary[dataset][i].update({'Plot': 'hist'})\n",
    "        else: summary[dataset][i].update({'Plot': None})\n",
    "\n",
    "        if fivenum[i] == True:\n",
    "            summary[dataset][i].update({'Summary': True})\n",
    "        else: summary[dataset][i].update({'Summary': False})\n",
    "\n",
    "    # add the maps to the lookup dictionary\n",
    "    categorical_counter = 0\n",
    "    for column in labels:\n",
    "        if dataset == 'casualties':\n",
    "            if categorical_counter == 2: \n",
    "                summary[dataset][column]['Map'] = VARIABLE_LOOKUP[35]\n",
    "                categorical_counter += 1\n",
    "                continue\n",
    "            if categorical_counter == 10:\n",
    "                summary[dataset][column]['Map'] = VARIABLE_LOOKUP[48]\n",
    "                categorical_counter += 1\n",
    "                continue\n",
    "            if categorical_counter == 11:\n",
    "                summary[dataset][column]['Map'] = VARIABLE_LOOKUP[47]\n",
    "                categorical_counter += 1\n",
    "                continue\n",
    "        \n",
    "        summary[dataset][column]['Map'] = VARIABLE_LOOKUP[start_at+categorical_counter]\n",
    "        categorical_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, start_at in zip(list(range(3)), [2, 37, 22]):\n",
    "    initialise_summary(TABLENAMES[i], SUMMARY, LABELS[TABLENAMES[i]], PLOTTING[TABLENAMES[i]], FIVENUM[TABLENAMES[i]], start_at=start_at)"
   ]
  },
  {
   "source": [
    "We also want to have a nice mapping for the time and date. We do this manually, since it is not defined in the given variable lookup excel."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time mapping\n",
    "SUMMARY['accidents'][11]['Map'] = {i: f\"{i}-{i+1}\" for i in range(24)}\n",
    "SUMMARY['accidents'][11]['Map'][-1] = 'N/A'\n",
    "\n",
    "# date mapping\n",
    "months = ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']\n",
    "SUMMARY['accidents'][9]['Map'] = {i+1: months[i] for i in range(12)}\n",
    "\n",
    "# no. casualties and vehicles\n",
    "SUMMARY['accidents'][7]['Map'] = {i: i for i in np.unique(DATA_LEEDS['accidents']['Number_of_Vehicles'])}\n",
    "SUMMARY['accidents'][8]['Map'] = {i: i for i in np.unique(DATA_LEEDS['accidents']['Number_of_Casualties'])}"
   ]
  },
  {
   "source": [
    "## Numerical Analysis\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniques_and_counts(data):\n",
    "    uniques, counts = np.unique(data, return_counts=True)\n",
    "\n",
    "    return uniques, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fivenumsummary(data):\n",
    "    return np.percentile(data[data >= 0], [0, 25, 50, 75, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary_categorical(summary, data): #\n",
    "    for column in range(len(summary)):\n",
    "        uniques, counts = get_uniques_and_counts(data[summary[column]['Name']])\n",
    "        summary[column]['No_Uniques'] = len(uniques) # get the number of uniques for each variable\n",
    "            \n",
    "        if summary[column]['Plot'] == 'bar': # get the counts for each uniques for all categorical attributes\n",
    "            if len(uniques) < 100:\n",
    "                summary[column]['Uniques'] = {uniques[i]: counts[i] for i in range(len(uniques))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary_numerical(summary, data):\n",
    "    for column in range(len(summary)):\n",
    "        if summary[column]['Summary'] == True:\n",
    "            summary[column]['Five_Number_Summary'] = get_fivenumsummary(data[summary[column]['Name']])\n",
    "        if summary[column]['Plot'] == 'hist' or summary[column]['Summary'] == True:\n",
    "            summary[column]['Data'] = np.array(data[summary[column]['Name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    compute_summary_categorical(SUMMARY[dataset], DATA_LEEDS[dataset])\n",
    "    compute_summary_numerical(SUMMARY[dataset], DATA_LEEDS[dataset])"
   ]
  },
  {
   "source": [
    "## Saving Numerical Reports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_numerical_report(summary, path, filename, save_to='csv'):\n",
    "    summary_dataframe = pd.DataFrame(summary)\n",
    "\n",
    "    try: os.makedirs(path)\n",
    "    except: None\n",
    "\n",
    "    if save_to == 'csv': summary_dataframe.to_csv(f'{path}/summary_{filename}.csv')\n",
    "    elif save_to == 'json': summary_dataframe.to_json(f'{path}/summary_{filename}.json')\n",
    "    else: raise NameError(f\"'{save_to}' not defined. Try saving to 'csv' or 'json' format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    save_numerical_report(SUMMARY[dataset], path=PATH['reports'][dataset] + 'numerical_summary/', filename=dataset,save_to='csv')\n",
    "    save_numerical_report(SUMMARY[dataset], path=PATH['reports'][dataset] + 'numerical_summary/', filename=dataset,save_to='json')"
   ]
  },
  {
   "source": [
    "## Visualisation\n",
    "---\n",
    "In this section, we are visualising the results of the single variable analysis in all three datasets - both within the Jupyter and per export into 'reports/figures' for later use."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot(data, keep_missing_values=True):\n",
    "    # create figure and axes (with padding for better exporting)\n",
    "    fig = plt.figure(figsize=(32,18))\n",
    "    ax = fig.add_axes([.15,.15,.7,.7])\n",
    "    \n",
    "    # variables depending on missing_values variable\n",
    "    if keep_missing_values:\n",
    "        x = list(data['Uniques'].keys())\n",
    "        y = list(data['Uniques'].values())\n",
    "        title = title = f\"Distribution: {data['Name'].replace('_', ' ')} (with missing values)\"\n",
    "        color = 'darkred'\n",
    "        yticks = list(data['Uniques'].keys())\n",
    "    \n",
    "    else: \n",
    "        if -1 in list(data['Uniques'].keys()):\n",
    "            x = list(data['Uniques'].keys())[1:]\n",
    "            y = list(data['Uniques'].values())[1:]\n",
    "            yticks = list(data['Uniques'].keys())[1:]\n",
    "        else: \n",
    "            x = list(data['Uniques'].keys())\n",
    "            y = list(data['Uniques'].values())\n",
    "            yticks = list(data['Uniques'].keys())\n",
    "\n",
    "        title = f\"Distribution: {data['Name'].replace('_', ' ')} (without missing values)\"\n",
    "        color = 'darkblue'\n",
    "        \n",
    "    spaced_ticks = [i for i in range(len(yticks))]\n",
    "    # plot bar \n",
    "    ax.barh(spaced_ticks, y, align='center', color=color)\n",
    "\n",
    "    # set title\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "\n",
    "    # format axis-labels\n",
    "    ax.set_xlabel('Number of Accidents')\n",
    "    try: # account for 0% datasets\n",
    "        ax.set_xlim(0, 1.15*max(y)) \n",
    "    except: None\n",
    "\n",
    "    ax.set_yticks(spaced_ticks)\n",
    "    ax.tick_params(axis='y', which='major', pad=10)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    try: \n",
    "        y_labels = [data['Map'][i] for i in x] # use lookup from xls \n",
    "        ax.set_yticklabels([textwrap.fill(label, 10) for label in y_labels])\n",
    "    except: None # account for variables that do not have lookup mapping\n",
    "\n",
    "    # insert counts and percentages as text next to the corresponding bars\n",
    "    for x_cord, y_cord in zip(spaced_ticks,y):\n",
    "        ax.text(y_cord, x_cord, f'{y_cord} ({str(100*round(y_cord/sum(y), 3))[:5]}%)' , color='black', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(data, keep_missing_values=True):\n",
    "    # create figure and axes (with padding for better exporting)\n",
    "    fig = plt.figure(figsize=(16,9))\n",
    "    ax = fig.add_axes([.1,.1,.8,.8])\n",
    "\n",
    "    if keep_missing_values:\n",
    "        title = f\"Distribution: {data['Name'].replace('_', ' ')} (with missing values)\"\n",
    "        data = data['Data']\n",
    "        color = 'darkred'\n",
    "\n",
    "    else: \n",
    "        title = f\"Distribution: {data['Name'].replace('_', ' ')} (without missing values)\"\n",
    "        data = data['Data'][(data['Data'] != -1)] # masking out -1\n",
    "        color = 'darkblue'\n",
    "\n",
    "    ax.hist(data, bins=50, color=color)\n",
    "\n",
    "    # set title\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "\n",
    "    # format axis-labels\n",
    "    ax.set_ylabel('Number of Accidents')\n",
    "    ax.set_xlabel('Age')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot(data):\n",
    "    # create figure and axes (with padding for better exporting)\n",
    "    fig = plt.figure(figsize=(16,9))\n",
    "    ax = fig.add_axes([.1,.1,.8,.8])\n",
    "\n",
    "    ax.boxplot(data['Data']);\n",
    "\n",
    "    ax.set_title(f\"Boxplot of {data['Name'].replace('_', ' ')}\", fontweight='bold')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "source": [
    "## Saving Figures\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figure(figure, path, column_index, column_name, save_to='pdf'):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except: None\n",
    "    figure.savefig(f'{path}/{column_index}_{column_name}.{save_to}')\n",
    "\n",
    "    print(f\"Saved: '{column_index}_{column_name}.{save_to}' to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_sva(summary, dataset_name, missing_values):\n",
    "    for column in range(len(summary)):\n",
    "        if summary[column]['Plot'] == 'bar':\n",
    "            # create barplot\n",
    "            fig = barplot(summary[column], keep_missing_values=missing_values)\n",
    "\n",
    "            # set path to save to \n",
    "            if missing_values: path = PATH['reports'][dataset_name] + 'single_variable_analysis/' + 'with_missing_values'\n",
    "            else: path = PATH[\"reports\"][dataset_name] + 'single_variable_analysis/' + 'without_missing_values'\n",
    "\n",
    "            # save to path\n",
    "            save_figure(fig, path, column_index=column, column_name=summary[column]['Name'], dataset_name=dataset_name, save_to='pdf')\n",
    "\n",
    "        elif summary[column]['Plot'] == 'hist':\n",
    "            # create histogram\n",
    "            fig = histogram(summary[column], keep_missing_values=missing_values)\n",
    "\n",
    "            # set path to save to \n",
    "            if missing_values: path = PATH['reports'][dataset_name] + 'single_variable_analysis/' + 'with_missing_values'\n",
    "            else: path = PATH[\"reports\"][dataset_name] + 'single_variable_analysis/' + 'without_missing_values'\n",
    "\n",
    "            save_figure(fig, path, column_index=column, column_name=summary[column]['Name'], dataset_name=dataset_name, save_to='pdf')\n",
    "\n",
    "        if summary[column]['Summary']:\n",
    "            # create boxplot\n",
    "            fig = boxplot(summary[column])\n",
    "\n",
    "            # set path to save to \n",
    "            path = PATH['reports'][dataset_name] + 'single_variable_analysis/' + 'boxplots'\n",
    "            \n",
    "            save_figure(fig, path, column_index=column, column_name=summary[column]['Name'], save_to='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "for dataset in TABLENAMES:\n",
    "    save_all_sva(SUMMARY[dataset], dataset, missing_values=True)\n",
    "    save_all_sva(SUMMARY[dataset], dataset, missing_values=False)"
   ]
  },
  {
   "source": [
    "# Associations (TASK 2)\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Categorical/ Numerical Analysis (Swarm Plots)\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_scatterplot(data, summary_categorical_variable, summary_numerical_variable, _kind='svarm', _exclude=100):\n",
    "    name_categorical, name_numerical = summary_categorical_variable['Name'], summary_numerical_variable['Name']\n",
    "    data_categorical, data_numerical = data[name_categorical], data[name_numerical]\n",
    "    \n",
    "    data_to_plot = np.array([data_categorical, data_numerical]).T\n",
    "    \n",
    "    # masking\n",
    "    without_missing_values = (data_to_plot[:,0] > -1) & (data_to_plot[:,1] > -1) # we first mask out all data records where either of the two observed attributes has missing values\n",
    "\n",
    "    uniques, counts = np.unique(data_categorical, return_counts=True)\n",
    "    under_100 = [uniques[i] for i in range(len(counts)) if counts[i] < _exclude] # we then also mask out \"irrelevant\" \n",
    "    exclude_small_values = ~np.isin(data_categorical, under_100)\n",
    "\n",
    "    final_mask = (without_missing_values) & (exclude_small_values)\n",
    "    data_to_plot = data_to_plot[final_mask]\n",
    "    \n",
    "    if _kind == 'svarm':\n",
    "        fig = sns.catplot(x = name_categorical, y = name_numerical, data=pd.DataFrame(data_to_plot, columns=[name_categorical, name_numerical]), kind='swarm', height=8.27, aspect=16/9, legend=True);\n",
    "        try:\n",
    "            fig.set_xticklabels([summary_categorical_variable['Map'][i] for i in [j for j in uniques if j not in under_100]]);\n",
    "        except: None\n",
    "        plt.xticks(rotation=45)\n",
    "        fig.fig.suptitle(f'Categorical Scatterplot for {name_categorical.replace(\"_\", \" \")} and {name_numerical.replace(\"_\", \" \")}', fontweight='bold')\n",
    "    elif _kind == 'violin':\n",
    "        fig = sns.catplot(x = name_categorical, y = name_numerical, data=pd.DataFrame(data_to_plot, columns=[name_categorical, name_numerical]), kind='violin', height=8.27, aspect=16/9, legend=True);\n",
    "        try:\n",
    "            fig.set_xticklabels([summary_categorical_variable['Map'][i] for i in [j for j in uniques if j not in under_100]]);\n",
    "        except: None\n",
    "        plt.xticks(rotation=45)\n",
    "        fig.fig.suptitle(f'Categorical Scatterplot for {name_categorical.replace(\"_\", \" \")} and {name_numerical.replace(\"_\", \" \")}', fontweight='bold')\n",
    "    else: raise NameError(f\"type = '{kind}'' is not defined. Try 'svarm' or 'violin'\")\n",
    "\n",
    "    return fig;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_association_test(data, marker_variable_summary, relational_variable_summary):\n",
    "    name1, name2 = marker_variable_summary['Name'], relational_variable_summary['Name']\n",
    "    data1, data2 = data[name1], data[name2]\n",
    "\n",
    "    # crosstab\n",
    "    observed_pd = pd.crosstab(data1, data2, rownames = [name1], colnames = [name2]).T\n",
    "    observed = observed_pd.to_numpy()\n",
    "\n",
    "    chiVal, pVal, df, expected = chi2_contingency(observed)\n",
    "    chiVal, pVal, df, expected.astype(int)\n",
    "    V = np.sqrt( (chiVal / observed.sum() ) / (min(observed.shape)-1) )\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=len(np.unique(data1)), figsize=(32, 16), constrained_layout=True)\n",
    "    fig.suptitle(f\"Association of {name1.replace('_', ' ')} and {name2.replace('_', ' ')} (chiVal: {round(chiVal, 2)}, pVal: {round(pVal, 2)}, V: {round(V, 2)})\", fontweight='bold', fontsize=16)\n",
    "    \n",
    "\n",
    "    labels1 = [marker_variable_summary['Map'][i] for i in np.unique(data1)]\n",
    "    labels2 = [relational_variable_summary['Map'][int(i)] for i in np.unique(data2)]\n",
    "    x = np.array(labels2)\n",
    "\n",
    "    for i, ax in enumerate(axes[0]):\n",
    "        ax.plot(x, observed[:,i], 'ro-', label='Observed')\n",
    "        ax.plot(x, expected[:,i], 'bo-', label='Expected')\n",
    "        if i==0: \n",
    "            ax.set_ylabel('No. of Accidents')\n",
    "            ax.legend(loc='best');\n",
    "        ax.set_title(labels1[i])\n",
    "        ax.set_xticks(x)\n",
    "\n",
    "    for i, ax in enumerate(axes[1]):\n",
    "        ax.plot(x, observed[:,i]/expected[:,i], 'go-')\n",
    "        ax.plot(x, np.ones(x.shape), 'k:')\n",
    "        \n",
    "        if i==0: \n",
    "            ax.set_ylabel('Observed/Expected')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels2)\n",
    "        fig.autofmt_xdate(rotation=45)\n",
    "    \n",
    "    return (fig, round(V,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY['casualties'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_association_test(DATA_LEEDS['casualties'], SUMMARY['casualties'][7], SUMMARY['casualties'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_scatterplot(DATA_LEEDS['casualties'], SUMMARY['casualties'][13], SUMMARY['casualties'][5], _kind='svarm', _exclude=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?save_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, V = categorical_association_test(DATA_LEEDS['accidents'], SUMMARY['accidents'][6], SUMMARY['accidents'][11])\n",
    "path = PATH['reports']['accidents'] + 'associations/' + 'chi_squared/' + SUMMARY['accidents'][6]['Name']\n",
    "save_figure(fig, path, column_index=f\"{V}_{6}_{11}\", column_name=f\"{SUMMARY['accidents'][6]['Name']}_{SUMMARY['accidents'][11]['Name']}\", save_to='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_categorical_scatters(data, summary, dataset_name):\n",
    "    for i in range(len(summary)):\n",
    "        if summary[i]['Plot'] == 'hist':\n",
    "            for j in range(len(summary)):\n",
    "                if summary[j]['Plot'] == 'bar':\n",
    "                    fig = categorical_scatterplot(data, summary[j], summary[i], _kind='svarm')\n",
    "\n",
    "                    path = PATH['reports'][dataset_name] + 'associations/' + 'categorical_scatters'\n",
    "\n",
    "                    save_figure(fig, path, column_index=f\"{i}_{j}\", column_name=f\"{summary[i]['Name']}_{summary[j]['Name']}\", save_to='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKERS = {}\n",
    "MARKERS['accidents'] = [6, 7, 8] # severity, #vehicles, #casualties\n",
    "MARKERS['vehicles'] = []\n",
    "MARKERS['casualties'] = [7] # severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_categorical_associations(data, summary, dataset_name):\n",
    "    for i in MARKERS[dataset_name]:\n",
    "        for j in range(len(summary)):\n",
    "            try: \n",
    "                if summary[j]['Map'] and j not in MARKERS[dataset_name] + [5, 12, 13]: # local authority highway and local authority district \n",
    "                    fig, V = categorical_association_test(data, summary[i], summary[j])\n",
    "\n",
    "                    path = PATH['reports'][dataset_name] + 'associations/' + 'chi_squared/' + summary[i]['Name']\n",
    "                    save_figure(fig, path, column_index=f\"{V}_{i}{j}\", column_name=f\"{summary[i]['Name']}_{summary[j]['Name']}\")\n",
    "            except: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "for dataset in TABLENAMES:\n",
    "    save_all_categorical_scatters(DATA_LEEDS[dataset], SUMMARY[dataset], dataset)\n",
    "    save_all_categorical_associations(DATA_LEEDS[dataset], SUMMARY[dataset], dataset)"
   ]
  },
  {
   "source": [
    "# Spatial Visualisation (TASK 3)\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Create Map of Leeds\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leeds_map = folium.Map(location=[53.8008, -1.5491], tiles=\"Stamen Terrain\", initial_zoom = 5)\n",
    "leeds_map"
   ]
  },
  {
   "source": [
    "## Plot all Accidents in Leeds onto Map\n",
    "--- "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_point(map, location, color, fill):\n",
    "    folium.Circle(\n",
    "            radius=30,\n",
    "            location=[location[0], location[1]],\n",
    "            color=color,\n",
    "            fill=True).add_to(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(DATA_LEEDS['accidents']['Accident_Severity'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_accidents(data, focus='Accident_Severity'):\n",
    "    uniques = np.unique(data[focus])\n",
    "    no_uniques = len(uniques)\n",
    "    # colors = [list(np.random.choice(range(256), size=3)) for _ in range(no_uniques)]\n",
    "    colors = ['black', 'red', 'green']\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(no_uniques):\n",
    "            if data[focus].iloc[i] == uniques[j]:\n",
    "                plot_point(leeds_map, (data['Latitude'].iloc[i], data['Longitude'].iloc[i]), color=colors[j], fill=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_accidents(DATA_LEEDS['accidents'], focus='Accident_Severity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leeds_map"
   ]
  },
  {
   "source": [
    "## Save Map Visualisation\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_map(map_, index, name):\n",
    "    try:\n",
    "        os.makedirs(f'../reports/maps')\n",
    "    except: None\n",
    "\n",
    "    map_.save(f'../reports/maps/{index}_{name}_map.html')"
   ]
  },
  {
   "source": [
    "# Bike Safety in Leeds (TASK 4)\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "https://ecf.com/news-and-events/news/data-collection-basis-better-road-safety"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## External Dataset\n",
    "---\n",
    "Obtained from: https://roadtraffic.dft.gov.uk/local-authorities/63 on 22.02.2021 (10AM)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTERNAL['road_flow'] = pd.read_csv(PATH['data_external'] + FILENAME['road_flow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTERNAL['road_flow'].drop(columns=['count_point_id', 'direction_of_travel', 'count_date', 'hour', 'region_id', 'region_name', 'local_authority_id', 'local_authority_name', 'road_name', 'road_type', 'start_junction_road_name', 'end_junction_road_name', 'easting', 'northing', 'latitude', 'longitude','link_length_km', 'link_length_miles','hgvs_2_rigid_axle', 'hgvs_3_rigid_axle', 'hgvs_3_or_4_articulated_axle', 'hgvs_5_articulated_axle', 'hgvs_6_articulated_axle'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for year in np.unique(EXTERNAL['road_flow']['year']):\n",
    "    mask = EXTERNAL['road_flow']['year'] == year\n",
    "\n",
    "    try: data['years'].append(year)\n",
    "    except: data['years'] = [year]\n",
    "\n",
    "    for i in range(1,EXTERNAL['road_flow'].shape[1]-1):\n",
    "        try: \n",
    "            data[list(EXTERNAL['road_flow'])[i]].append(sum(EXTERNAL['road_flow'].iloc[:,i][mask]))\n",
    "        except: data[list(EXTERNAL['road_flow'])[i]] = [sum(EXTERNAL['road_flow'].iloc[:,i][mask])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure and axes (with padding for better exporting)\n",
    "fig = plt.figure(figsize=(16,9))\n",
    "ax = fig.add_axes([.15,.15,.7,.7])    \n",
    "\n",
    "years = data['years']\n",
    "labels = list(EXTERNAL['road_flow'])[1:-1]\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    ax.plot(years, data[labels[i]], 'o-', label=labels[i])\n",
    "\n",
    "ax.set_xticks(years); ax.set_xticklabels(years);\n",
    "ax.set_title('Annual Leeds Traffic Flow by Vehicle (2000-2019)', fontweight='bold', fontsize=16)\n",
    "ax.set_xlabel('Years'); ax.set_ylabel('No. of Vehicles Counted');\n",
    "ax.get_yaxis().set_major_formatter(\n",
    "    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "ax.legend();\n",
    "\n",
    "save_figure(fig, PATH['reports']['external'], column_index='1', column_name='annual_leeds_traffic_flow', save_to='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Filtering for Bike Accidents\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes = DATA_LEEDS['vehicles'][DATA_LEEDS['vehicles']['Vehicle_Type'] == 1]\n",
    "bike_accidents_indexes = set(bikes['Accident_Index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    DATA_LEEDS_BIKES[dataset] = DATA_LEEDS[dataset][DATA_LEEDS[dataset]['Accident_Index'].isin(bike_accidents_indexes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Accident_Index  Location_Easting_OSGR  Location_Northing_OSGR  \\\n",
       "41055  2019136111190               435904.0                425850.0   \n",
       "41058  2019136111836               429149.0                431736.0   \n",
       "41060  2019136120357               428760.0                432723.0   \n",
       "41087  2019136170679               432347.0                430836.0   \n",
       "41097  2019136180248               429558.0                431566.0   \n",
       "...              ...                    ...                     ...   \n",
       "44549  2019136CF0784               425916.0                435496.0   \n",
       "44578  2019136CI0859               439303.0                432396.0   \n",
       "44586  2019136CI1657               431062.0                430364.0   \n",
       "44608  2019136CK2067               430765.0                433846.0   \n",
       "44669  2019136CV0723               436853.0                442515.0   \n",
       "\n",
       "       Longitude   Latitude  Police_Force  Accident_Severity  \\\n",
       "41055  -1.457300  53.727837            13                  3   \n",
       "41058  -1.559127  53.781158            13                  2   \n",
       "41060  -1.564938  53.790050            13                  2   \n",
       "41087  -1.510690  53.772881            13                  3   \n",
       "41097  -1.552937  53.779607            13                  3   \n",
       "...          ...        ...           ...                ...   \n",
       "44549  -1.607873  53.815122            13                  2   \n",
       "44578  -1.404956  53.786425            13                  3   \n",
       "44586  -1.530234  53.768716            13                  3   \n",
       "44608  -1.534394  53.800029            13                  3   \n",
       "44669  -1.440932  53.877548            13                  2   \n",
       "\n",
       "       Number_of_Vehicles  Number_of_Casualties Date  ...  \\\n",
       "41055                   2                     1    1  ...   \n",
       "41058                   2                     1    1  ...   \n",
       "41060                   2                     1    1  ...   \n",
       "41087                   2                     1    1  ...   \n",
       "41097                   2                     1    1  ...   \n",
       "...                   ...                   ...  ...  ...   \n",
       "44549                   2                     1   12  ...   \n",
       "44578                   2                     1   12  ...   \n",
       "44586                   2                     1   12  ...   \n",
       "44608                   2                     1   12  ...   \n",
       "44669                   2                     1   12  ...   \n",
       "\n",
       "       Pedestrian_Crossing-Human_Control  \\\n",
       "41055                                  0   \n",
       "41058                                  0   \n",
       "41060                                  0   \n",
       "41087                                  0   \n",
       "41097                                  0   \n",
       "...                                  ...   \n",
       "44549                                  0   \n",
       "44578                                  0   \n",
       "44586                                  0   \n",
       "44608                                  2   \n",
       "44669                                  0   \n",
       "\n",
       "      Pedestrian_Crossing-Physical_Facilities  Light_Conditions  \\\n",
       "41055                                       0                 1   \n",
       "41058                                       0                 4   \n",
       "41060                                       0                 1   \n",
       "41087                                       0                 6   \n",
       "41097                                       1                 1   \n",
       "...                                       ...               ...   \n",
       "44549                                       0                 1   \n",
       "44578                                       0                 1   \n",
       "44586                                       0                 4   \n",
       "44608                                       4                 4   \n",
       "44669                                       0                 1   \n",
       "\n",
       "      Weather_Conditions  Road_Surface_Conditions  Special_Conditions_at_Site  \\\n",
       "41055                  1                        1                           0   \n",
       "41058                  1                        1                           0   \n",
       "41060                  1                        1                           0   \n",
       "41087                  1                        1                           0   \n",
       "41097                  1                        1                           0   \n",
       "...                  ...                      ...                         ...   \n",
       "44549                  1                        2                           0   \n",
       "44578                  1                        2                           0   \n",
       "44586                  2                        2                           0   \n",
       "44608                  1                        1                           0   \n",
       "44669                  1                        1                           0   \n",
       "\n",
       "       Carriageway_Hazards  Urban_or_Rural_Area  \\\n",
       "41055                    0                    2   \n",
       "41058                    0                    1   \n",
       "41060                    0                    1   \n",
       "41087                    0                    1   \n",
       "41097                    0                    1   \n",
       "...                    ...                  ...   \n",
       "44549                    0                    1   \n",
       "44578                    0                    2   \n",
       "44586                    0                    1   \n",
       "44608                    0                    1   \n",
       "44669                    0                    2   \n",
       "\n",
       "       Did_Police_Officer_Attend_Scene_of_Accident  LSOA_of_Accident_Location  \n",
       "41055                                            1                  E01011636  \n",
       "41058                                            1                  E01011366  \n",
       "41060                                            1                  E01033013  \n",
       "41087                                            2                  E01011470  \n",
       "41097                                            1                  E01011364  \n",
       "...                                            ...                        ...  \n",
       "44549                                            1                  E01011281  \n",
       "44578                                            2                  E01011402  \n",
       "44586                                            1                  E01011472  \n",
       "44608                                            1                  E01033031  \n",
       "44669                                            1                  E01011713  \n",
       "\n",
       "[238 rows x 32 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accident_Index</th>\n      <th>Location_Easting_OSGR</th>\n      <th>Location_Northing_OSGR</th>\n      <th>Longitude</th>\n      <th>Latitude</th>\n      <th>Police_Force</th>\n      <th>Accident_Severity</th>\n      <th>Number_of_Vehicles</th>\n      <th>Number_of_Casualties</th>\n      <th>Date</th>\n      <th>...</th>\n      <th>Pedestrian_Crossing-Human_Control</th>\n      <th>Pedestrian_Crossing-Physical_Facilities</th>\n      <th>Light_Conditions</th>\n      <th>Weather_Conditions</th>\n      <th>Road_Surface_Conditions</th>\n      <th>Special_Conditions_at_Site</th>\n      <th>Carriageway_Hazards</th>\n      <th>Urban_or_Rural_Area</th>\n      <th>Did_Police_Officer_Attend_Scene_of_Accident</th>\n      <th>LSOA_of_Accident_Location</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>41055</th>\n      <td>2019136111190</td>\n      <td>435904.0</td>\n      <td>425850.0</td>\n      <td>-1.457300</td>\n      <td>53.727837</td>\n      <td>13</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>E01011636</td>\n    </tr>\n    <tr>\n      <th>41058</th>\n      <td>2019136111836</td>\n      <td>429149.0</td>\n      <td>431736.0</td>\n      <td>-1.559127</td>\n      <td>53.781158</td>\n      <td>13</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>E01011366</td>\n    </tr>\n    <tr>\n      <th>41060</th>\n      <td>2019136120357</td>\n      <td>428760.0</td>\n      <td>432723.0</td>\n      <td>-1.564938</td>\n      <td>53.790050</td>\n      <td>13</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>E01033013</td>\n    </tr>\n    <tr>\n      <th>41087</th>\n      <td>2019136170679</td>\n      <td>432347.0</td>\n      <td>430836.0</td>\n      <td>-1.510690</td>\n      <td>53.772881</td>\n      <td>13</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>E01011470</td>\n    </tr>\n    <tr>\n      <th>41097</th>\n      <td>2019136180248</td>\n      <td>429558.0</td>\n      <td>431566.0</td>\n      <td>-1.552937</td>\n      <td>53.779607</td>\n      <td>13</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>E01011364</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>44549</th>\n      <td>2019136CF0784</td>\n      <td>425916.0</td>\n      <td>435496.0</td>\n      <td>-1.607873</td>\n      <td>53.815122</td>\n      <td>13</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>12</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>E01011281</td>\n    </tr>\n    <tr>\n      <th>44578</th>\n      <td>2019136CI0859</td>\n      <td>439303.0</td>\n      <td>432396.0</td>\n      <td>-1.404956</td>\n      <td>53.786425</td>\n      <td>13</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>12</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>E01011402</td>\n    </tr>\n    <tr>\n      <th>44586</th>\n      <td>2019136CI1657</td>\n      <td>431062.0</td>\n      <td>430364.0</td>\n      <td>-1.530234</td>\n      <td>53.768716</td>\n      <td>13</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>12</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>E01011472</td>\n    </tr>\n    <tr>\n      <th>44608</th>\n      <td>2019136CK2067</td>\n      <td>430765.0</td>\n      <td>433846.0</td>\n      <td>-1.534394</td>\n      <td>53.800029</td>\n      <td>13</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>12</td>\n      <td>...</td>\n      <td>2</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>E01033031</td>\n    </tr>\n    <tr>\n      <th>44669</th>\n      <td>2019136CV0723</td>\n      <td>436853.0</td>\n      <td>442515.0</td>\n      <td>-1.440932</td>\n      <td>53.877548</td>\n      <td>13</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>12</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>E01011713</td>\n    </tr>\n  </tbody>\n</table>\n<p>238 rows × 32 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "DATA_LEEDS_BIKES['accidents']"
   ]
  },
  {
   "source": [
    "Questions:\n",
    "- What are dangerous locations for bikers?\n",
    "- Who is endangered? (Sex, Age, ...)\n",
    "- What is the factor that incrases the accident severity with bikes the most?\n",
    "- "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Single Variable Analysis\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, start_at in zip(list(range(3)), [2, 37, 22]):\n",
    "    initialise_summary(TABLENAMES[i], SUMMARY_BIKES, LABELS[TABLENAMES[i]], PLOTTING[TABLENAMES[i]], FIVENUM[TABLENAMES[i]], start_at=start_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time mapping\n",
    "SUMMARY_BIKES['accidents'][11]['Map'] = {i: f\"{i}-{i+1}\" for i in range(24)}\n",
    "SUMMARY_BIKES['accidents'][11]['Map'][-1] = 'N/A'\n",
    "\n",
    "# date mapping\n",
    "months = ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']\n",
    "SUMMARY_BIKES['accidents'][9]['Map'] = {i+1: months[i] for i in range(12)}\n",
    "\n",
    "# no. casualties and vehicles\n",
    "SUMMARY_BIKES['accidents'][7]['Map'] = {i: i for i in np.unique(DATA_LEEDS['accidents']['Number_of_Vehicles'])}\n",
    "SUMMARY_BIKES['accidents'][8]['Map'] = {i: i for i in np.unique(DATA_LEEDS['accidents']['Number_of_Casualties'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    compute_summary_categorical(SUMMARY_BIKES[dataset], DATA_LEEDS_BIKES[dataset])\n",
    "    compute_summary_numerical(SUMMARY_BIKES[dataset], DATA_LEEDS_BIKES[dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    save_numerical_report(SUMMARY[dataset], path=PATH['reports']['external'] + 'numerical_summaries/', filename=dataset, save_to='csv')\n",
    "    save_numerical_report(SUMMARY[dataset], path=PATH['reports']['external'] + 'numerical_summaries/', filename=dataset, save_to='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'save_all_sva' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-e91942b6af62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTABLENAMES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msave_all_sva\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSUMMARY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'save_all_sva' is not defined"
     ]
    }
   ],
   "source": [
    "%%capture \n",
    "for dataset in TABLENAMES:\n",
    "    save_all_sva(SUMMARY[dataset], dataset, missing_values=False)"
   ]
  }
 ]
}