{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "---\n",
    "### First inspection of dataset, data filtering and cleaning\n",
    "\n",
    "Creation: 05.02.2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # used to store the datasets\n",
    "import numpy as np  # used for numerical calculations on individual columns"
   ]
  },
  {
   "source": [
    "## Constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path lookup dictionary to store the relative paths from the directory containing the jupyter notebooks to important directories in the project\n",
    "PATH = {}\n",
    "PATH[\"data_raw\"] = \"../data/raw/\"\n",
    "PATH[\"data_interim\"] = \"../data/interim/\"\n",
    "PATH[\"data_processed\"] = \"../data/processed/\"\n",
    "PATH[\"data_external\"] = \"../data/external/\"\n",
    "PATH[\"references\"] = \"../data/references\"\n",
    "\n",
    "# filename lookup dictionary storing the most relevant filenames\n",
    "FILENAME = {}\n",
    "FILENAME[\"accidents\"] = \"Road Safety Data - Accidents 2019.csv\"\n",
    "FILENAME[\"casualties\"] = \"Road Safety Data - Casualties 2019.csv\"\n",
    "FILENAME[\"vehicles\"] = \"Road Safety Data- Vehicles 2019.csv\" # the original dataset has a small typing mistake\n",
    "FILENAME[\"variable_lookup\"] = \"variable lookup.xls\"\n",
    "\n",
    "DATA_RAW = {}\n",
    "\n",
    "DATA_LEEDS = {}\n",
    "\n",
    "# list of internal names for datasets\n",
    "TABLENAMES = [\"accidents\", \"casualties\", \"vehicles\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the Dataset\n",
    "---\n",
    "Starting of the data analysis, we import the given three datasets into a 'pandas' dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all three datasets using pandas into the predefined dictionary 'DATA_RAW' where the key corresponds to the internal dataset name\n",
    "for name in TABLENAMES:\n",
    "    DATA_RAW[name] = pd.read_csv(PATH['data_raw'] + FILENAME[name])"
   ]
  },
  {
   "source": [
    "## Initial Sanity Check\n",
    "---\n",
    "Before continuing with the data analysis, we want to make sure that the dataset is clean. We do this in a several ways. \n",
    "- (a) Are there multiple indexes in 'accidents.csv'\n",
    "- (b) We check if there are indexes in the sub datasets 'raw_data_vehicles' and 'raw_data_casualties' that are not in the central dataset 'raw_accidents.csv'\n",
    "- (c) We check the missing values in each column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Mulitple Indexes\n",
    "We are first evaluating if there are mulitple indexes in the big dataset 'accidents.csv'. This is a first, very basic metric to determine whether the data in the dataset has been inputted correctly. In this case it is even more important since the accident indexes link the main dataset 'accidents.csv' to the two sub datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we check if there are mulitple indexes in the accidents dataset\n",
    "DATA_RAW['accidents'].shape[0] == len(set(DATA_RAW['accidents']['Accident_Index']))"
   ]
  },
  {
   "source": [
    "Perfect. There do not seem to be any multiple indexes. Each row in the dataset 'accidents.csv' seems to refer to a unique accident that we can reference in the two sub datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Wrong Indexes in Sub-Datasets\n",
    "It would be bad if there is information on vehicles and casualties in the two subsets that are referenced by an Accident ID that is not in the main dataset 'accidents.csv'. So we check for those using the below code."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_indexes_in_subset(sub_dataset, _in=DATA_RAW['accidents']):\n",
    "    \"\"\" \n",
    "    Helper-Function to evaluate whether there are indexes in the two linked sub datasets that do not appear in the main dataset.\n",
    "\n",
    "    Parameters:\n",
    "        sub_dataset         : pd.DataFrame\n",
    "        _in                 : pd.DataFrame\n",
    "    Return:\n",
    "        #Wrong Indexes      : int (None if len() == 0)\n",
    "    \"\"\"\n",
    "    \n",
    "    accidents_indexes = set(_in['Accident_Index'])\n",
    "    wrong_indexes = [i for i in set(sub_dataset['Accident_Index']) if i not in accidents_indexes]\n",
    "\n",
    "    if len(wrong_indexes) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return len(wrong_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the wrong indexes for each sub dataset\n",
    "for dataset in TABLENAMES:\n",
    "    if dataset != 'accidents':\n",
    "        print(f\"#Missing indexes in {dataset.capitalize()}: {check_indexes_in_subset(DATA_RAW[dataset], DATA_RAW['accidents'])}\")"
   ]
  },
  {
   "source": [
    "\"This is rather bad. Roughly 21.500 indexes in the 'vehicles.csv' raw dataset are linking to an accident that is not registered in the 'accidents.csv', 19.300 in the 'casualties.csv' are linking casualties to accidents that are not registered int the 'accidents.csv'. If a similar behavior can be observed in the dataset that is filtered for Leeds, we have to think about how to deal with those obvious wrong input of data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Check for missing values\n",
    "Here we check in all columns of all datasets to see if there are missing values (empty string) to get a feeling on which columns we need to further do processing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_column_for_missing_value(column):\n",
    "    \"\"\" \n",
    "    For a column of a pd.DataFrame (One-Dimensional Array) the fucntion returns the number of Null-Values that correspond to the          number of missing values in the column.\n",
    "\n",
    "    Parameters:\n",
    "        column              : pd.DataFrame\n",
    "    Return:\n",
    "        #Null Values        : int\n",
    "    \"\"\"\n",
    "\n",
    "    return sum(column.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_columns(data):\n",
    "    \"\"\"\n",
    "    For a dataset provided as a pd.DataFrame the function returns an informative string about each column containing null values,         namely the number of missing values, the column index and the variable name of the column.\n",
    "\n",
    "    Parameters:\n",
    "        data                : pd.DataFrame\n",
    "    Return:\n",
    "        Informative String for each column containing null values, else None\n",
    "    \"\"\"\n",
    "\n",
    "    for column in range(data.shape[1]):\n",
    "        if check_column_for_missing_value(data.iloc[:,column]) != 0:\n",
    "            print(f'{check_column_for_missing_value(data.iloc[:,column])} ({data.columns[column]}({column}))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    print(dataset.capitalize())\n",
    "    check_all_columns(DATA_RAW[dataset])\n",
    "    print('\\n')"
   ]
  },
  {
   "source": [
    "That's not bad! There are no missing values in both sub datasets. \n",
    "It seems like in the accidents dataset, there are 28 accidents that have no information on their location. This only gets important for our analysis if one of those accidents is located in Leeds, then we would need to deal with this issue later. The LSOA Metric - which is another measure of the accident location of the accident - hasn't been registered for 5714 accidents. This is not important for our analysis, since we will use the longitude and latitude to plot the accidents' location. \n",
    "There are, however, 63 accidents for which the time of accident is not registered. If any of those accidents are located in Leeds, we have to deal with them later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering \n",
    "---\n",
    "Next, we filter the main dataset 'accidents.csv' for the city of interest 'Leeds', which can be identified by several variables in the dataset. We here chose the column 'Local Authority (District)', where 'Leeds' is identified as 204. The resulting, filtered dataframe is saved into a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS[\"accidents\"] = DATA_RAW['accidents'][DATA_RAW['accidents']['Local_Authority_(District)'] == 204]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the other two datasets cannont be identified by the variable attributes, but need to be filtered through the unique accident indexes that we can obtain from our filtered dataframe of accidents in 'Leeds'. We obtain a list of all accident indexes of the accidents that occured in Leeds and use this index list to filter both the 'vehicles.csv' and 'casualties.csv' datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "leeds_indexes = list(DATA_LEEDS['accidents']['Accident_Index']) # we can do it with list because we know that all indexes are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS[\"casualties\"] = DATA_RAW['casualties'][DATA_RAW['casualties']['Accident_Index'].isin(leeds_indexes)]\n",
    "DATA_LEEDS[\"vehicles\"] = DATA_RAW['vehicles'][DATA_RAW['vehicles']['Accident_Index'].isin(leeds_indexes)]"
   ]
  },
  {
   "source": [
    "## Saving Filtered Data\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    DATA_LEEDS[dataset].to_csv(PATH['data_interim'] + FILENAME[dataset], index=False)"
   ]
  },
  {
   "source": [
    "## Sanity Check for Leeds\n",
    "--- \n",
    "Now, that we filtered the dataset, we do the exact same snaity checks that we performed on the raw datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Mulitple Indexes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['accidents'].shape[0] == len(set(DATA_LEEDS['accidents']['Accident_Index']))"
   ]
  },
  {
   "source": [
    "### Wrong Indexes in Sub-Datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    if dataset != 'accidents':\n",
    "        print(f\"#Missing indexes in {dataset.capitalize()}: {check_indexes_in_subset(DATA_LEEDS[dataset], DATA_LEEDS['accidents'])}\")"
   ]
  },
  {
   "source": [
    "### Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    print(dataset.capitalize())\n",
    "    check_all_columns(DATA_LEEDS[dataset])\n",
    "    print('\\n')"
   ]
  },
  {
   "source": [
    "Perfect. None of the sanity checks reports any problems on our dataset. At this point we could export the dataset and work on the filtered ones. However, we are making some adjustments in the below section to make our analysis easier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data\n",
    "---\n",
    "In this section, the 'Date' and 'Time' attributes in the 'accidents.csv' module will be cleaned for easy use in the single variable analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.array(DATA_LEEDS['accidents']['Time'])\n",
    "for i in range(len(time)):\n",
    "    try: \n",
    "        time[i] = time[i][:2]\n",
    "    except:\n",
    "        time[i] = '-1'\n",
    "\n",
    "DATA_LEEDS['accidents']['Time'] = time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = np.array(DATA_LEEDS['accidents']['Date'])\n",
    "\n",
    "\n",
    "for i in range(len(date)):\n",
    "    date[i] = int(date[i][3:5])\n",
    "\n",
    "DATA_LEEDS['accidents']['Date'] = date"
   ]
  },
  {
   "source": [
    "### 2nd Road Class\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_road_class = np.array(DATA_LEEDS['accidents']['2nd_Road_Class'])\n",
    "\n",
    "for i in range(len(second_road_class)):\n",
    "    if second_road_class[i] == -1:\n",
    "        second_road_class[i] = 0\n",
    "\n",
    "DATA_LEEDS['accidents']['2nd_Road_Class'] = second_road_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "---\n",
    "For each of the datasets, we want to get a first good impression of its size and the information it stores. To gain this information, we print out each of the datasets, and get a summary of each of the columns and the uniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accidents\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['accidents'].shape # prints out the number of columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['accidents'] # prints out an overview of the dataframe (and the number of rows and columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['accidents'].nunique() # prints out the column names and the corresponding number of unique values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, that the main dataset 'accidents_processed.csv' stores all recorded accidents in 2019 in Leeds. It consist of 1451 columns (which leads to 1450 recorded accidents) and has 32 columns providing more detailed information about the accident. The different variables and the number of its unique values can be studied in the output of the above cell. We see, that we can differentiate the attributes as follows:\n",
    "- Categorical Attributes (Most of the columns are categorical)\n",
    "- Geographical Attributes (There are several measures of the location of the accident)\n",
    "- Time Attribute (Each accident specifies a date and time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicles\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['vehicles'].shape # prints out the number of columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['vehicles'] # prints out an overview of the dataframe (and the number of rows and columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['vehicles'].nunique() # prints out the column names and the corresponding number of unique values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, that the side dataset 'vehicles_processed.csv' provides more detailed information about all vehicles involved in each of the accidents. It consist of 2688 columns (which leads to 2688 records on involved vehicles) and has 23 columns providing more detailed information about the vehicle. The different variables and the number of its unique values can be studied in the output of the above cell. We see, that we can differentiate the attributes as follows:\n",
    "- Linking Attributes (Accident Indexes link the vehicles to the accidents dataset and the vehicle references the casualties)\n",
    "- Categorical Attributes (Most of the columns are categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casualties\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['casualties'].shape # prints out the number of columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['casualties'] # prints out an overview of the dataframe (and the number of rows and columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEEDS['casualties'].nunique() # prints out the column names and the corresponding number of unique values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, that the side dataset 'casualties_processed.csv' provides more detailed information about the casualties of all lethal accidents. It consist of 1908 columns (which leads to 1907 records on casualties) and has 16 columns providing more detailed information about the vehicle. The different variables and the number of its unique values can be studied in the output of the above cell. We see, that we can differentiate the attributes as follows:\n",
    "- Linking Attributes (Accident Indexes link the vehicles to the accidents dataset and the vehicle references the casualties)\n",
    "- Categorical Attributes (Most of the columns are categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Processed Datasets\n",
    "--- \n",
    "Finally, we export the processed datasets into a new subfolder. From now on, all Jupyter Notebooks will work with those processed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in TABLENAMES:\n",
    "    DATA_LEEDS[dataset].to_csv(PATH['data_processed'] + FILENAME[dataset], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}